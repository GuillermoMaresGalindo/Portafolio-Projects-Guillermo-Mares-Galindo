{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, sum, max, min, when, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e0099",
   "metadata": {},
   "source": [
    "## Creating a SparkSession\n",
    "\n",
    "The SparkSession is the entry point for all Spark functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e84983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Introduction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c73b7",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "We'll work with a dataset of data science salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the salaries dataset\n",
    "df = spark.read.csv(\"Data/salaries.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# show first few rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f4815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stats\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f010f",
   "metadata": {},
   "source": [
    "## Basic Transformations\n",
    "\n",
    "### Select and Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc949331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select specific columns\n",
    "df.select(\"job_title\", \"salary_in_usd\", \"experience_level\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rows - high salary jobs\n",
    "high_salary = df.filter(col(\"salary_in_usd\") > 150000)\n",
    "print(f\"Jobs with salary > $150k: {high_salary.count()}\")\n",
    "high_salary.select(\"job_title\", \"salary_in_usd\", \"company_location\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple conditions\n",
    "senior_remote = df.filter(\n",
    "    (col(\"experience_level\") == \"SE\") & \n",
    "    (col(\"remote_ratio\") == 100)\n",
    ")\n",
    "print(f\"Senior fully remote jobs: {senior_remote.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ddc4b",
   "metadata": {},
   "source": [
    "### Adding and Modifying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa64be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column\n",
    "df_with_monthly = df.withColumn(\"monthly_salary_usd\", col(\"salary_in_usd\") / 12)\n",
    "df_with_monthly.select(\"job_title\", \"salary_in_usd\", \"monthly_salary_usd\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional column with when\n",
    "df_salary_level = df.withColumn(\n",
    "    \"salary_category\",\n",
    "    when(col(\"salary_in_usd\") < 50000, \"Low\")\n",
    "    .when(col(\"salary_in_usd\") < 100000, \"Medium\")\n",
    "    .when(col(\"salary_in_usd\") < 150000, \"High\")\n",
    "    .otherwise(\"Very High\")\n",
    ")\n",
    "df_salary_level.select(\"job_title\", \"salary_in_usd\", \"salary_category\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f2ae4",
   "metadata": {},
   "source": [
    "## Aggregations\n",
    "\n",
    "Grouping and summarizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average salary by experience level\n",
    "df.groupBy(\"experience_level\") \\\n",
    "    .agg(avg(\"salary_in_usd\").alias(\"avg_salary\")) \\\n",
    "    .orderBy(\"avg_salary\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1356cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple aggregations\n",
    "df.groupBy(\"experience_level\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"salary_in_usd\").alias(\"avg_salary\"),\n",
    "    min(\"salary_in_usd\").alias(\"min_salary\"),\n",
    "    max(\"salary_in_usd\").alias(\"max_salary\")\n",
    ").orderBy(\"avg_salary\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top job titles by count\n",
    "df.groupBy(\"job_title\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd86d7b",
   "metadata": {},
   "source": [
    "## SQL Queries\n",
    "\n",
    "Spark allows you to run SQL queries directly on DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register dataframe as a temp view\n",
    "df.createOrReplaceTempView(\"salaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sql query\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        job_title,\n",
    "        COUNT(*) as job_count,\n",
    "        ROUND(AVG(salary_in_usd), 2) as avg_salary\n",
    "    FROM salaries\n",
    "    WHERE experience_level = 'SE'\n",
    "    GROUP BY job_title\n",
    "    HAVING COUNT(*) >= 5\n",
    "    ORDER BY avg_salary DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b21aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salary trends by year\n",
    "yearly_trend = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        work_year,\n",
    "        COUNT(*) as total_jobs,\n",
    "        ROUND(AVG(salary_in_usd), 2) as avg_salary\n",
    "    FROM salaries\n",
    "    GROUP BY work_year\n",
    "    ORDER BY work_year\n",
    "\"\"\")\n",
    "yearly_trend.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75207f",
   "metadata": {},
   "source": [
    "## Remote Work Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18adb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote work distribution\n",
    "remote_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN remote_ratio = 0 THEN 'On-site'\n",
    "            WHEN remote_ratio = 50 THEN 'Hybrid'\n",
    "            WHEN remote_ratio = 100 THEN 'Fully Remote'\n",
    "        END as work_type,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(AVG(salary_in_usd), 2) as avg_salary\n",
    "    FROM salaries\n",
    "    GROUP BY remote_ratio\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "remote_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f72de2",
   "metadata": {},
   "source": [
    "## Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e57a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: save aggregated results (commented to avoid creating files)\n",
    "# df.groupBy(\"job_title\").count().write.csv(\"output/job_counts\", header=True, mode=\"overwrite\")\n",
    "print(\"To save: df.write.csv('path', header=True)\")\n",
    "print(\"Or parquet: df.write.parquet('path')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1564632",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8373d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17341148",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "- **SparkSession**: Entry point for Spark applications\n",
    "- **DataFrames**: Distributed collection of data organized into named columns\n",
    "- **Transformations**: select, filter, withColumn, when\n",
    "- **Aggregations**: groupBy, agg, count, avg, min, max\n",
    "- **SQL**: createOrReplaceTempView, spark.sql()\n",
    "\n",
    "### When to Use Spark\n",
    "\n",
    "- Data doesn't fit in memory on a single machine\n",
    "- Need distributed processing across a cluster\n",
    "- Working with big data pipelines (ETL)\n",
    "- Integration with data lakes and warehouses"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
